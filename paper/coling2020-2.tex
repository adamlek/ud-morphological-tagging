\nonstopmode
% 
% File coling2020.tex
%
% Contact: feiliu@cs.ucf.edu & liang.huang.sh@gmail.com
%% Based on the style files for COLING-2018, which were, in turn,
%% Based on the style files for COLING-2016, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2020}
\usepackage{times}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage{wrapfig}
\pgfplotsset{every tick label/.append style={font=\small}}
\pgfplotsset{/pgfplots/error bars/error bar style={thin}}
\usepackage{pgfplotstable}
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
\usepackage{colortbl}

\newcommand\jp[1]{(\textbf{JP:} #1)}
\newcommand\adam[1]{(\textbf{Adam:} #1)}
\newcommand\citet{\newcite}
\newcommand\citep{\cite}
\newcommand\citeyear{\shortcite}
\newcommand\softmax{\mathsf{softmax}}

%\setlength\titlebox{5cm}
\colingfinalcopy % Uncomment this line for the final submission

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Composing Byte-Pair Encodings for Morphological Sequence Classification}


\author{Adam Ek \and Jean-Philippe Bernardy\\
	Centre for Linguistic Theory and Studies in Probability,\\
	Department of Philosophy, Linguistics and Theory of Science,\\
	University of Gothenburg,\\
	\texttt{\{adam.ek, jean-philippe.bernardy\}@gu.se}}
\date{}
    
\begin{document}
\maketitle
\begin{abstract}
          Byte-pair encodings is a method for splitting a word into
     sub-word tokens, a language model then assigns contextual
     representations separately to each of these tokens.  In this
     paper, we evaluate four different methods of composing such
     sub-word representations into word representations. We evaluate
     the methods on morphological sequence classification,
     the task of predicting grammatical features of a word. Our
     experiments reveal that using an RNN to compute word
     representations is consistently more effective than the other
     methods tested across a sample of eight languages with different
     typology and varying numbers of byte-pair tokens per word.
\end{abstract}

	\section{Introduction}
	\label{intro}

            After its introduction, the Transformer model
     \citep{vaswani2017attention} has emerged as the dominant
     architecture for statistical language models, displacing
     recurrent neural networks, in particular, the LSTM and its
     variants. The Transformer owes its success to several factors,
     including the availability of pretrained models, which
     effectively yield rich contextual word embeddings. Such
     embeddings can be used as is (for so-called \emph{feature extraction}),
     or the pre-trained models can be finetuned to specific
     tasks.

    	At the same time as Transformer models became popular, the
     tokenization of natural language texts have shifted away from
     methods explicitly oriented towards words or morphemes. Rather,
     statistical approaches are favoured: strings of
     characters are split into units which are not necessarily meaningful
     linguistically, but rather have statistically balanced
     frequencies. For example, the word ``scientifically" may be
     composed of the tokens: ``scient", ``ifical", ``ly" --- here the
     central token does not correspond to a morpheme.
        %
             That is, rather than identifying complete words or
     morphemes, one aims to find relatively large sub-word units
     occurring significantly often, while maximizing the coverage of
     the corpus (the presence of the ``out of vocabulary'' token is
     minimized). Approaches for composing words from sub-word units
     have focused on combining character $n$-grams
     \citep{bojanowski2017enriching}, while other approaches have
     looked at splitting words into \textit{roots} and
     \textit{morphemes}
     \citep{el2012orthographic,chaudhary2018adapting,xu2017implicitly},
     and then combining them.

        In this paper, we consider Byte-Pair Encodings
     (BPE) \citep{sennrich2015neural}.  BPE has been popularized by
     its usage in translation and the BERT Transformer model
     \citep{devlin2018bert}.  The BPE algorithm does not specifically
     look for either character $n$-grams or morphs, but rather it aims
     at splitting a corpus $\mathcal{C}$ into $N$ tokens, where $N$ is
     user defined.
    %
        Even though BPE is not grounded in morphosyntactic theory, the
     characteristics of the sub-word units generated by BPE will be
     directly influenced by morphosyntactic patterns in a language. In
     particular, it is reasonable to expect that the statistical
     characteristics of BPE to be different between languages with
     different typologies.
    %
     One issue with this tokenization scheme is that models based on
     BPE provide vector representations for the BPE tokens (which we
     call \emph{token embeddings} from now on), while one is typically
     interested in representations for the semantically meaningful
     units in the original texts, \emph{words}. In sum,
     one wants to combine \emph{token embeddings} into
     \emph{word embeddings}.

     Our main goal is to explore how to best combine token embeddings  in
     the context of sequence classification on words, that is, the
     task of assigning a label to every word in a sentence.
    %
          Coming back to our example, we must combine the token
     embeddings assigned to the BPE tokens "scient", "ifical" and "ly" to
     form a word representation of ``scientifically'' (as a vector)
     which we can then assign a label to.


                    	To our knowledge, this is a little-studied
     problem. For the original BERT model \citet{devlin2018bert}
     simply state that for named entity recognition the first sub-word
     token is used as the word representation.  For
     morphological sequence classification \citet{kondratyuk2019cross,kondratyukstraka}
     report that only small differences in performance were found between averaging,
     taking the maximum value or first sub-word token.
    %
    In this paper we explore the problem in further
     detail and identify the effect that different methods have on the
     final performance of a model.
     %
                     Additionally, with the increased interest in
     multilingual NLP it becomes important to explore how different
     computational methods perform cross-linguistically.
        That is, because languages are different morphosyntactically,
     one can expect various computational methods not to be uniformly
     effective.


    \section{Task}

            To investigate composition methods for token embeddings
     we focus on the task of morphological sequence
     classification. The task is to assign a tag to a word that
     represent its grammatical features, such as gender, number and so
     on.
    %
            In addition to the word-form, the system can use
     information from context words as cues. While the grammatical
     features primarily are given by the word-form, useful information
     is also found in the context.


                Thus, we have to identify $k$ different tags for a
     word, each with $C_i$ possible classes, making the task a
     multi-class classification problem. We simplify the
     classification problem by combining the different tags into a
     composite tag with up to $\prod _i^k C_i$ classes (instead of
     making $k$ separate predictions).
    %
    %                       This task is suitable for our goal as the
    % output space is larger, ranging from 100 to 1000 possible tags
    % for a word, depending on the grammatical features present in the
    % language\footnote{For practical reasons, we only consider tag
    % combinations observed in the dataset}, see \cref{tab:data}. A
    % system must efficiently encode information about the structure of
    % the target words as well as the context words to be able to
    % predict the correct grammatical features.
%
        This task is suitable for our goal as the output space is
     large, ranging from 100 to 1000 possible tags for a word,
     depending on the grammatical features present in the
     language\footnote{For practical reasons, we only consider tag
     combinations observed in the dataset}, and is directly linked to
     the affixes in the word-form. A system must efficiently encode
     information about the structure of the target words as well as
     the context words to be able to predict the correct grammatical
     features.
    
    \section{Data}
    
            For both training and testing data, we use the Universal
     Dependencies dataset \citep{nivre2018} annotated with the
     UniMorph schema \citep{mccarthy2018marrying}.  We are mainly
     interested in how the accuracy is influenced by different
     composition methods, but also consider the type of morphology a
     language uses as a factor in this task.
%
    With this in mind, we consider both
     languages that use agglutinative morphology where each morpheme
     is mapped to one and only one grammatical feature, and
     languages that use fusional morphology where a morpheme can be
     mapped to one or more grammatical features. 
%
           	The fusional languages that we consider are Arabic, Czech,
     Polish and Spanish, and the agglutinative languages that we
     consider are Finnish, Basque, Turkish, and Estonian.  We show the
     size, the average number of BPE tokens per word, and the number of
     morphological tags for each treebank in \cref{tab:data}.
    
    
    \begin{table}
		\centering
		\begin{tabular}{l|lrrrrr}
			Language & Typology & $\frac{\textrm{BPE}}{\textrm{word}}$ & Tags & Train & Validation & Test \\
			\hline
			Basque-BDT      & Agglutinative & 1.79 & 919 & 97336 & 12206 & 11901 \\
			Finnish-TDT     & Agglutinative & 1.98 & 591 & 161791 & 19876 & 20541 \\
			Turkish-IMST    & Agglutinative & 1.73 & 1056 & 46417 & 5708 & 5734 \\
			Estonian-EDT    & Agglutinative & 1.86 & 512 & 346986 & 43434 & 43825 \\
    		Spanish-AnCora  & Fusional & 1.25 & 177 & 439925 & 55196 & 54449 \\ 
            Arabic-PADT     & Fusional & 1.39 & 300 & 225494 & 28089 & 28801  \\
			Czech-CAC       & Fusional & 1.77 & 990 & 395043 & 50087 & 49253 \\
			Polish-LFG      & Fusional & 1.75 & 634 & 104730 & 13161 & 13076 \\
        \end{tabular}
        \caption{\label{tab:data} Treebank statistics showing the
     language typology, average number of BPE tokens per word, the
     number of (composite) morphological tags and the size of the
     datasets in terms of words.}
	\end{table}
    
            The fusional languages were chosen such that two of them
     (Czech and Polish) have a higher BPE per word ratio than the
     other two (Arabic and Spanish). We make this choice because one
     factor that impacts the accuracy obtained by a composition method
     may be the BPE per word ratio.  By having both fusional and
     agglutinative languages with similar BPE per word ratio we can
     take this variable into account properly in our analysis.
        
	\section{Method}
	\label{method}
        	In this section we present the model used for
     morphological sequence classification, the methods that we use to
     compose token embeddings, and how the model is
     trained. \footnote{Our code is available at:
     \url{https://github.com/adamlek/ud-morphological-tagging}}

	\subsection{Model}

                Our model is composed of three components, each of
     them detailed below. First, the input sequence of BPE tokens is
     fed to a Transformer model, which yields a contextual vector
     representation for each BPE token. The contextual information
     here is the surrounding BPE tokens in the sentence.  Then, the
     token embeddings are combined using a composition module, which
     we vary for the purpose of evaluating each variant. This
     component yields one embedding per original word. Then we pass
     the word embeddings through a bidirectional LSTM, which is
     followed by two dense layers with GELU
     \citep{hendrycks2016gaussian} activation. These dense layers act
     on each word embedding separately (but share parameters across
     words).  An outline of the model is presented in
     \cref{fig:model}, where $f$ represents the different methods we
     use to combine token embeddings.

	\subsubsection{Underlying Transformer Model}
         To extract a embeddings for each BPE token, we use the
     XLM-RoBERTa \cite{conneau2019unsupervised} model\footnote{We use
     the huggingface implementation
     \url{https://huggingface.co/transformers/model_doc/xlmroberta.html}}. XLM-R
     is a masked language model based on the Transformer, specifically
     RoBERTa \citep{liu2019roberta}, and trained on data from 100
     different languages, using a shared vocabulary of 250000 BPE
     tokens. All the languages that we test are included in the XLM-R
     model. In this experiment we use the XLM-R\textsubscript{base} model
     with 250M parameters. It has 12 encoder layers, 12 attention
     heads and use 768 dimensions for its hidden size.

	\subsubsection{Feature extraction}
        \label{sec:bpe-features}

    The XLM-R model uses 12 layers to compute a vector representation
     for a BPE token. It has been shown in previous research
     \citep{kondratyukstraka,raganato2018analysis,liu2019linguistic}
     that the different layers of the Transformer model encode
     different types of information.

    \begin{figure}%[h!]
    \centering
	\includegraphics[scale=0.5]{single-step-final.pdf}
            \caption{\label{fig:model} Model outline for one input. A
     word $w_n$ is tokenized into $k$ BPE tokens. The Transformer
     model produces one embedding per token per layer.  We then calculate
     a weighted sum over the layers to obtain one representation
     per token.  The resulting token embeddings are then passed to
     a composition function $f$ that combines the $k$ different token
     embeddings into a word embedding. The word embedding is then passed
     to an LSTM followed by a dense prediction layer. }
	\end{figure}

                        To take advantage of this variety, we compute
     token embeddings as a weighted sum of the layer representation
     \citep{kondratyukstraka}, using a weight vector $w$, of size $l$,
     where $l$ is the number of layers in the Transformer model.  The
     weight vector $w$ is initialized from a normal distribution of
     mean $0$ and standard deviation $1$. If $r_{ji}$ is the layer
     representation at layer $j$ and token position $i$, we calculate
     the weighted sum as follows:
    \begin{equation}
		x_i = \sum_{j=1}^{l} \softmax(w)_j r_{ji}
	\end{equation}

     Consequently, in end-to-end training, the optimiser will find a
     weight for extracting information from each layer
     ($\softmax(w)_j$) which maximizes performance.

     \subsubsection{Composition of BPE token embeddings}
          The weighted sum yields a token embedding for each BPE token. We
     proceed to combine them into words as they appear in the data.
      The model that we use to combine token embeddings is as
     follows. For each sentence we extract $n$ token embeddings $x^0$ to
     $x^{n-1}$ from XLM-R\textsubscript{base}, and then align them to words.
    %
                    We then pass all token embeddings in a word to a
     function $f$ which combines the tokens into a word embedding.

    We consider four methods for composing token embeddings: taking the first token embedding,
     summation, averaging, and using an RNN. Taking the first token
     embedding, summation and averaging have been used in previous work
     \citep{sachan2020syntax,kondratyuk2019cross,devlin2018bert}, but
     using an RNN has not been explored before to our knowledge.

        \paragraph{First:} The first method is the standard one used by
     \citet{devlin2018bert}, which is to use the first token embedding in a word.
    
             \paragraph{Sum:} For the Sum method, we use an
     element-wise sum. That is, we calculate the vector sum of the
     token embeddings. Assuming that we have $T$ token embeddings in a
     word $X$ (the word is a matrix of size $(T,768)$), for dimension
     $i$ we calculate the word embedding by summing the token
     embeddings:
	\begin{equation}
	f(X)_i = \sum_{j=1}^{T} x_i^j
	\end{equation}

         \paragraph{Mean:} In the mean method we calculate the sum as
     above and divide by the number of BPE tokens in the word. Thus,
     for word $X$ we calculate the word embedding by averaging over
     the sum:
	\begin{equation}
	f(X)_{i} = \frac{1}{T}\sum_{j=1}^{T} x_i^j
	\end{equation}
	
	
     \paragraph{RNN:} For this method we employ a bidirectional
     LSTM to compose the token embeddings. For each word, we
     pass the sequence of token embeddings through an LSTM and use the
     final output as the word representation.

     \subsubsection{Word-level features and classification}

    The above methods of composing BPE tokens produce one contextual
     embedding per word.  We then pass the word embeddings through an LSTM
     to take into account the word contexts.  While the BPE token
     embeddings are already contextual, they are conditioned on the BPE
     token context, not word context.  We pass the hidden states for
     each word to a residual connection with the pre-LSTM
     representation. We then pass this to two dense layers with GELU
     activation followed by a dense layer that computes class-scores
     for each word. We then use a softmax layer to assign
     probabilities and compute the loss accordingly.

        Commonly, systems analyzing morphology use character
     embeddings as an additional source of information. We opted not
     to include character embeddings because this would obfuscate the
     effect of the composition method and may mask some of the effects
     of the different methods.
    
	\subsubsection{Label smoothing}
    	Given that many of the languages have a large number of
     morphological tags, we want to prevent the model from growing
     overconfident for certain classes. To address this issue we
     introduce label smoothing \cite{szegedy2016rethinking}, that is,
     instead of the incorrect classes having $0\%$ probability and the
     correct class $100\%$ probability we let each of the incorrect
     classes have a small probability.

         Let $\alpha$ be our smoothing value, in our model we follow
     \citep{kondratyukstraka} and use $\alpha = 0.03$, and $C$ the
     number of classes, then given a one-hot encoded target vector $t$
     of size $C$, we calculate the smoothed probabilities as:
    \begin{equation}
        t_{smooth} = (1-\alpha)t + \frac{\alpha}{C}
    \end{equation}
    In words, we remove $\alpha$ from the correct class then
    distribute $\alpha$ uniformly among all classes.


    \subsection{Training}

            \begin{wraptable}{r}{4.8cm}
		\centering
		\begin{tabular}{lr} \\
			Parameter & Value \\
			\hline
			Epochs & 15 \\
			Batch size & 4 / 32 \\
			%Character representation size & 128 \\
            Word LSTM size & 768 \\
            Linear transform size & 1536 \\
			% optimizers
			Optimizer & Adam \\
			Learning rate & 0.001 \\
			Learning rate$_{xlmr}$ & \num{1e-06} \\
			% regularization
            Weight decay & 0.05 \\
			Label smoothing & 0.03 \\
            % described in text
            %Prediction dropout & 0.5 \\
			%Transformer dropout & 0.4 \\
            %Layer dropout & 0.1 \\
            %Input dropour & 0.2 \\
            %BPE dropout & 0.4 \\
            %Character dropout & 0.4 \\
		\end{tabular}
    		\caption{\label{tab:parameters} Hyperparameters used for
     training the model. Slashed indicates the value of a parameter
     when we finetune or extract features.}
	\end{wraptable}
    
     In our experiments we consider two possible training regimes. In
     the first regime we finetune the XLM-R model's parameters, in the
     second we only extract weights for BPE tokens, that is, we
     use the model as a feature extractor. In all cases, we use
     end-to-end training.
     
                             When finetuning the model we freeze the
     XLM-R parameters for the first epoch, effectively not finetuning
     at first. When training the model we use a cosine annealing
     learning rate \citep{loshchilov2016sgdr} with restarts every
     epoch, that is, the learning rate starts high then incrementally
     decreases to \num{1e-12} over $N$ steps, where $N$ is the
     number of batches in an epoch.  We use the Adam optimizer with
     standard parameters, with a learning rate of $0.001$ for layer
     importance parameter ($w$ in \cref{sec:bpe-features}), the
     parameters of the Word-LSTM, of the classification layer, and of
     the BPE-combination module (when an RNN is used).  For the
     Transformer parameters, we use a lower learning rate of
     \num{1e-6}.  We summarize the hyperparameters used in
     \cref{tab:parameters}.

                As an additional regularization in addition to weight
     decay and adaptive learning rate, we use dropout throughout the
     model.  Generally, we apply dropout before some feature is
     computed. Initial experiments revealed that a high dropout
     yielded the best results. We summarize the dropout used as:
    %
        We replace 20 percent of the BPE tokens with
     \texttt{<UNK>}. Then, we compute a weighted sum of the layer
     representations, to regularize this operation we apply dropout on
     layer representations with a probability of $0.1$, that is we set
     all representations in the layer to $0$. We then combine the
     token embeddings into word embeddings and apply a dropout of $0.4\%$,
     and pass these into the Word-LSTM. Before the contextualized
     representation is passed to the classification layer, we apply a
     dropout of $0.4\%$.
    
	\section{Results}
	\label{results}

             Even though our aim is to compare the relative
     performance of various BPE-combination methods rather than to
     improve on the state of the art in absolute terms, we compare our
     results against the baseline reported by
     \citet{mccarthy2019sigmorphon}. This comparison serves the
     purpose of checking that our system is generally sound.  In
     particular, the actual state of the art, as reported by
     \citet{mccarthy2019sigmorphon,kondratyuk2019cross}, uses treebank
     concatenation or other methods to incorporate information from
     all treebanks available in a language, which means that results
     are not reported on a strict per-treebank basis and thus our
     numbers are not directly comparable.
   %  The accuracy of our system is computed by checking the prediction
   %     of morphological tags. We report in \cref{tab:results_tokens}
   %     the result for each of the four different methods, and the
   %     two training regimes.
        We report the accuracy of prediction morphological tags for
     each of our composition methods, and for our two training regimes
     in \cref{tab:results_tokens}.

    \begin{table}%[h]
	%\small
	\centering
	\begin{tabular}{l|c|cccc|cccc}
		& & \multicolumn{4}{c}{Finetuning} & \multicolumn{4}{c}{Feature extraction} \\
		Treebank & Baseline & First & Sum & Mean & RNN & First & Sum & Mean & RNN \\
		\hline
		Basque-BDT      & .676 & .857 & .884 & .877 & \textbf{.901} & .759 & .789 & .780 & \textbf{.834} \\
		Finnish-TDT     & .751 & .961 & .958 & .960 & \textbf{.965} & .853 & .856 & .847 & \textbf{.899} \\
		Turkish-IMST    & .620 & .848 & .859 & .855 & \textbf{.884} & .742 & .741 & .735 & \textbf{.775} \\
		Estonian-EDT    & .740 & .956 & .955 & .955 & \textbf{.961} & .855 & .856 & .853 & \textbf{.901} \\
		Spanish-AnCora  & .842 & .977 & .977 & .977 & \textbf{.979} & .951 & .954 & .952 & \textbf{.962} \\
		Arabic-PADT     & .770 & .946 & .946 & .947 & \textbf{.951} & .920 & .923 & .920 & \textbf{.936} \\
		Czech-CAC       & .771 & .968 & .968 & .968 & \textbf{.975} & .863 & .887 & .881 & \textbf{.924} \\
		Polish-LFG      & .657 & .956 & .953 & .953 & \textbf{.959} & .828 & .844 & .840 & \textbf{.878} \\
        \hline
        Average         & .728 & .933 & .937 & .936 & \textbf{.946} & .846 & .856 & .851 & \textbf{.888} \\
	\end{tabular}
    	\caption{\label{tab:results_tokens} Accuracy for morphological
          tagging. We show scores both for finetuning the XLM-R model and
          extracting features.}
    \end{table}


                Our system performs better than the baseline. As a
     general trend we see that the RNN method tends to perform better
     than all other tested methods. This trend is consistent across
     both language families (agglutinative and fusional) and
     training regimes showing that, while the advantage of the RNN is
     small, it occurs consistently.
        %
            In general we find that finetuning yields higher accuracy
     than plain feature extraction, on average the difference is about
     $5.8$ percentage points.  This difference is to be expected when
     finetuning has 250M more parameters tuned to the task than the
     feature extraction.
    
                Focusing on the finetuning regime only, we see the
     largest benefits of the RNN method for Basque with an increased
     performance of $3.25$ points, and $2.7$ points for Turkish over
     using mean or averaging. The First method for Basque and Turkish
     performs worse with a decrease of $4.4$ percentage points
     for Basque and $3.6$ points for Turkish compared to the RNN method.
    %
        In the bare features extraction regime, we see a larger
     benefit for the RNN, of $3.7$ percentage points (Turkish) and
     $4.95$ points (Basque). Again, this is not unexpected: When
     finetuning the error rate is smaller, and therefore there is a
     smaller margin for a subsequent phase to yield and improvement.

    
	\begin{table}%[h]
	%\small
	\centering
	\begin{tabular}{l|cccc|cccc}
		 & \multicolumn{4}{c}{Finetuning} & \multicolumn{4}{c}{Feature extraction} \\
		Treebank & First & Sum & Mean & RNN & First & Sum & Mean & RNN  \\
		 \hline
		% agglutinative languages
        Basque-BDT      & .739 & .802 & .790 & \textbf{.835} & .657 & .715 & .703 & \textbf{.774} \\
		Finnish-TDT     & .940 & .946 & .946 & \textbf{.952} & .780 & .805 & .794 & \textbf{.861} \\ 
		Turkish-IMST    & .730 & .780 & .778 & \textbf{.818} & .653 & .683 & .664 & \textbf{.711} \\
		Estonian-EDT    & .938 & .939 & .939 & \textbf{.949} & .779 & .805 & .803 & \textbf{.868} \\
		% fusional languages
		Spanish-AnCora  & .956 & .961 & .959 & \textbf{.964} & .922 & .937 & .930 & \textbf{.947} \\
		Arabic-PADT     & .889 & .896 & .898 & \textbf{.907} & .902 & .909 & .906 & \textbf{.923}\\
		Czech-CAC       & .940 & .947 & .947 & \textbf{.959} & .786 & .849 & .840 & \textbf{.900} \\
		Polish-LFG      & .917 & .920 & .918 & \textbf{.927} & .696 & .761 & .752 & \textbf{.812} \\
        \hline
        Average         & .881 & .899 & .897 & \textbf{.913} & .772 & .808 & .799 & \textbf{.849} \\
	\end{tabular}
    \caption{\label{tab:results_large_tokens} Accuracy for
     morphological tagging on all words that are composed of two or
     more BPE tokens.}
\end{table}

            \Cref{tab:results_tokens} reports average accuracy for
     every word, including those which are only composed of a single
     BPE token. To highlight the strengths and weaknesses of each
     composition method, we also compute the accuracy for longer words
     only (composed of two or more BPE tokens). The results can be
     seen in \cref{tab:results_large_tokens}.  We see the same trend
     for accuracy on words that are composed of two or more BPE
     tokens, as in the overall accuracy, where the RNN outperforms all
     other methods. We can also see that the average increase in
     accuracy when using an RNN is larger. This holds both when
     finetuning or extracting bare features.
    %
        Given that the number of BPE tokens per word varies in the
     different languages, we also look at the accuracy of the
     different methods given the number of BPE tokens. We show
     per-language performance with the different methods in
     \cref{fig:bpe_lens}.
    
	\begin{figure}%[h!]
  	\centering
    \begin{tikzpicture} 
	\begin{axis}[
	name=,
	title={Finnish-TDT},
    xtick style={draw=none},
    ytick style={draw=none},
    xtick={1,2,3,4,5,6,7},
    xticklabels={1,2,3,4,5,6,$\ge$7},
	typeset ticklabels with strut,
	legend pos=south west,
	ymajorgrids=false,
        ymax=1.0,
        ymin={0.9},
	grid style=dashed,
	height=4cm
	]
		\addplot[
		color=green,
		mark=*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.969671597)
			(2, 0.94797361)
			(3, 0.945349627)
			(4, 0.926686217)
			(5, 0.959302326)
			(6, 0.972222222)
			(7, 0.95)
};
		\addplot[
		color=cyan,
		mark=triangle*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.972648618)
			(2, 0.950801131)
			(3, 0.94263408)
			(4, 0.926686217)
			(5, 0.968023256)
			(6, 0.953703704)
			(7, 0.95)
};
		\addplot[
		color=red,
		mark=square*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.976369895)+=(0,0.00288107757099608)-=(0,0.00288107757099608)
			(2, 0.954759661)+=(0,0.005610028700870861)-=(0,0.005610028700870861)
			(3, 0.950441276)+=(0,0.007875906986290072)-=(0,0.007875906986290072)
			(4, 0.933528837)+=(0,0.01540785138661897)-=(0,0.01540785138661897)
			(5, 0.970930233)+=(0,0.019118549510097702)-=(0,0.019118549510097702)
			(6, 0.972222222)+=(0,0.03801018391614076)-=(0,0.03801018391614076)
			(7, 0.95)+=(0,0.08447212917485333)-=(0,0.08447212917485333)
};
		\addplot[
		color=brown,
		mark=diamond*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.980091171)
			(2, 0.945900094)
			(3, 0.93856076)
			(4, 0.908113392)
			(5, 0.962209302)
			(6, 0.935185185)
			(7, 0.95)
};

        \end{axis}
	\end{tikzpicture}
\begin{tikzpicture} 
	\begin{axis}[
	name=,
	title={Basque-BDT},
        xtick style={draw=none},
        ytick style={draw=none},
        xtick={1,2,3,4,5,6,7},
        xticklabels={1,2,3,4,5,6,$\ge$7},
	typeset ticklabels with strut,
	legend pos=south west,
	ymajorgrids=false,
        ymax=1.0,
        ymin={0.6},
	grid style=dashed,
	height=4cm
	]
		\addplot[
		color=green,
		mark=*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.945468053)
			(2, 0.823694905)
			(3, 0.775357386)
			(4, 0.748051948)
			(5, 0.68)
			(6, 0.833333333)
			(7, 0.8)
};
		\addplot[
		color=cyan,
		mark=triangle*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.942496285)
			(2, 0.814004376)
			(3, 0.755616065)
			(4, 0.742857143)
			(5, 0.68)
			(6, 0.833333333)
			(7, 1.0)
};
		\addplot[
		color=red,
		mark=square*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.95141159)+=(0,0.005148305118481129)-=(0,0.005148305118481129)
			(2, 0.863394811)+=(0,0.01190991764618752)-=(0,0.01190991764618752)
			(3, 0.796460177)+=(0,0.020591939005449287)-=(0,0.020591939005449287)
			(4, 0.761038961)+=(0,0.0425432420184802)-=(0,0.0425432420184802)
			(5, 0.746666667)+=(0,0.09746065981141003)-=(0,0.09746065981141003)
			(6, 0.75)+=(0,0.22787810036435233)-=(0,0.22787810036435233)
			(7, 0.8)+=(0,0.31002797388117415)-=(0,0.31002797388117415)
};
		\addplot[
		color=brown,
		mark=diamond*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.946210996)
			(2, 0.764301344)
			(3, 0.713410483)
			(4, 0.65974026)
			(5, 0.6)
			(6, 0.666666667)
			(7, 0.8)
};

        \end{axis}
	\end{tikzpicture}
\begin{tikzpicture} 
	\begin{axis}[
	name=,
	title={Turkish-IMST},
        xtick style={draw=none},
        ytick style={draw=none},
        xtick={1,2,3,4,5,6,7},
        xticklabels={1,2,3,4,5,6,$\ge$7},
	typeset ticklabels with strut,
	legend pos=south west,
	ymajorgrids=false,
        ymax=1.0,
        ymin={0.6},
	grid style=dashed,
	height=4cm
	]
		\addplot[
		color=green,
		mark=*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.909405655)
			(2, 0.806088683)
			(3, 0.697416974)
			(4, 0.796511628)
			(5, 0.869565217)
			(6, 0.769230769)
			(7, 0.75)
};
		\addplot[
		color=cyan,
		mark=triangle*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.903346797)
			(2, 0.802117803)
			(3, 0.70295203)
			(4, 0.796511628)
			(5, 0.826086957)
			(6, 0.769230769)
			(7, 1.0)
};
		\addplot[
		color=red,
		mark=square*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.924985574)+=(0,0.008789990590271397)-=(0,0.008789990590271397)
			(2, 0.846459298)+=(0,0.018197041939921957)-=(0,0.018197041939921957)
			(3, 0.743542435)+=(0,0.03671374756645732)-=(0,0.03671374756645732)
			(4, 0.808139535)+=(0,0.058966217953330555)-=(0,0.058966217953330555)
			(5, 0.826086957)+=(0,0.15686382857803416)-=(0,0.15686382857803416)
			(6, 0.769230769)+=(0,0.21719589193854613)-=(0,0.21719589193854613)
			(7, 0.75)+=(0,0.3383902260894284)-=(0,0.3383902260894284)
};
		\addplot[
		color=brown,
		mark=diamond*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.922388921)
			(2, 0.75843812)
			(3, 0.65498155)
			(4, 0.715116279)
			(5, 0.695652174)
			(6, 0.769230769)
			(7, 0.75)
};

        \end{axis}
	\end{tikzpicture}
\begin{tikzpicture} 
	\begin{axis}[
	name=,
	title={Estonian-EDT},
        xtick style={draw=none},
        ytick style={draw=none},
        xtick={1,2,3,4,5,6,7},
        xticklabels={1,2,3,4,5,6,$\ge$7},
	typeset ticklabels with strut,
	legend pos=south west,
	ymajorgrids=false,
        ymax=1.0,
        ymin={0.9},
	grid style=dashed,
	height=4cm
	]
		\addplot[
		color=green,
		mark=*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.966596057)
			(2, 0.94286927)
			(3, 0.93281916)
			(4, 0.925671812)
			(5, 0.957522124)
			(6, 0.921052632)
			(7, 0.946808511)
};
		\addplot[
		color=cyan,
		mark=triangle*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.96667753)
			(2, 0.941765705)
			(3, 0.932007307)
			(4, 0.936535163)
			(5, 0.955752212)
			(6, 0.927631579)
			(7, 0.946808511)
};
		\addplot[
		color=red,
		mark=square*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.970506762)+=(0,0.002118846172644754)-=(0,0.002118846172644754)
			(2, 0.951867572)+=(0,0.0038703455288459482)-=(0,0.0038703455288459482)
			(3, 0.944793992)+=(0,0.006393466972485876)-=(0,0.006393466972485876)
			(4, 0.94053745)+=(0,0.011154986451904374)-=(0,0.011154986451904374)
			(5, 0.959292035)+=(0,0.01681968549237994)-=(0,0.01681968549237994)
			(6, 0.940789474)+=(0,0.04007961692831919)-=(0,0.04007961692831919)
			(7, 0.936170213)+=(0,0.05404871299247627)-=(0,0.05404871299247627)
};
		\addplot[
		color=brown,
		mark=diamond*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.970425289)
			(2, 0.942359932)
			(3, 0.929977674)
			(4, 0.928530589)
			(5, 0.953982301)
			(6, 0.927631579)
			(7, 0.936170213)
};

        \end{axis}
	\end{tikzpicture}
\begin{tikzpicture} 
	\begin{axis}[
	name=,
	title={Arabic-PADT},
        xtick style={draw=none},
        ytick style={draw=none},
        xtick={1,2,3,4,5,6,7},
        xticklabels={1,2,3,4,5,6,$\ge$7},
	typeset ticklabels with strut,
	legend pos=south west,
	ymajorgrids=false,
        ymax=1.0,
        ymin={0.75},
	grid style=dashed,
	height=4cm
	]
		\addplot[
		color=green,
		mark=*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.96220714)
			(2, 0.903356247)
			(3, 0.880846873)
			(4, 0.871595331)
			(5, 0.8)
};
		\addplot[
		color=cyan,
		mark=triangle*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.962904425)
			(2, 0.90578245)
			(3, 0.88183161)
			(4, 0.883268482)
			(5, 0.933333333)
};
		\addplot[
		color=red,
		mark=star,
                mark size=1.5,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.965600595)+=(0,0.0024381283986652067)-=(0,0.0024381283986652067)
			(2, 0.910230489)+=(0,0.007976150815817857)-=(0,0.007976150815817857)
			(3, 0.90201871)+=(0,0.012961727810939487)-=(0,0.012961727810939487)
			(4, 0.894941634)+=(0,0.03810358069342934)-=(0,0.03810358069342934)
			(5, 0.866666667)+=(0,0.18330014190315588)-=(0,0.18330014190315588)
};
		\addplot[
		color=brown,
		mark=diamond*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.964298996)
			(2, 0.896684189)
			(3, 0.873953717)
			(4, 0.86770428)
			(5, 0.933333333)
};

        \end{axis}
	\end{tikzpicture}
\begin{tikzpicture} 
	\begin{axis}[
	name=,
	title={Spanish-ANCORA},
        xtick style={draw=none},
        ytick style={draw=none},
        xtick={1,2,3,4,5,6,7},
        xticklabels={1,2,3,4,5,6,$\ge$7},
	typeset ticklabels with strut,
	legend pos=south west,
	ymajorgrids=false,
        ymax=1.0,
        ymin={0.7},
	grid style=dashed,
	height=4cm
	]
		\addplot[
		color=green,
		mark=*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.981325799)
			(2, 0.96449483)
			(3, 0.954242928)
			(4, 0.943152455)
			(5, 0.905660377)
			(6, 0.92)
			(7, 0.75)
};
		\addplot[
		color=cyan,
		mark=triangle*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.981233921)
			(2, 0.962999875)
			(3, 0.952163062)
			(4, 0.932816537)
			(5, 0.886792453)
			(6, 0.92)
			(7, 0.75)
};
		\addplot[
		color=red,
		mark=square*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.982267549)+=(0,0.0012411463013903851)-=(0,0.0012411463013903851)
			(2, 0.967360159)+=(0,0.0038992019513958173)-=(0,0.0038992019513958173)
			(3, 0.957154742)+=(0,0.008154258951432975)-=(0,0.008154258951432975)
			(4, 0.943152455)+=(0,0.023764512705614565)-=(0,0.023764512705614565)
			(5, 0.905660377)+=(0,0.08501108280383891)-=(0,0.08501108280383891)
			(6, 0.92)+=(0,0.1250821627692161)-=(0,0.1250821627692161)
			(7, 0.75)+=(0,0.3383902260894284)-=(0,0.3383902260894284)
};
		\addplot[
		color=brown,
		mark=diamond*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.981991915)
			(2, 0.9616295)
			(3, 0.944259567)
			(4, 0.919896641)
			(5, 0.886792453)
			(6, 0.88)
			(7, 0.75)
};

        \end{axis}
	\end{tikzpicture}
\begin{tikzpicture} 
	\begin{axis}[
	name=,
	title={Polish-LFG},
        xtick style={draw=none},
        ytick style={draw=none},
        xtick={1,2,3,4,5,6,7},
        xticklabels={1,2,3,4,5,6,$\ge$7},
	typeset ticklabels with strut,
	legend pos=south west,
	ymajorgrids=false,
        ymax=1.0,
        ymin={0.8},
	grid style=dashed,
	height=4cm
	]
		\addplot[
		color=green,
		mark=*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.969714687)
			(2, 0.930019881)
			(3, 0.905500705)
			(4, 0.912601626)
			(5, 0.921052632)
			(6, 0.888888889)
};
		\addplot[
		color=cyan,
		mark=triangle*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.971468662)
			(2, 0.926838966)
			(3, 0.907616361)
			(4, 0.914634146)
			(5, 0.828947368)
			(6, 0.888888889)
};
		\addplot[
		color=red,
		mark=square*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.975444341)+=(0,0.0032933193579321087)-=(0,0.0032933193579321087)
			(2, 0.942345924)+=(0,0.009152642925086161)-=(0,0.009152642925086161)
			(3, 0.910437236)+=(0,0.014925424898063413)-=(0,0.014925424898063413)
			(4, 0.908536585)+=(0,0.025763757285900076)-=(0,0.025763757285900076)
			(5, 0.842105263)+=(0,0.08322533572401784)-=(0,0.08322533572401784)
			(6, 0.888888889)+=(0,0.22927231782530055)-=(0,0.22927231782530055)
};
		\addplot[
		color=brown,
		mark=diamond*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.977081384)
			(2, 0.929224652)
			(3, 0.905500705)
			(4, 0.902439024)
			(5, 0.828947368)
			(6, 0.888888889)
};

        \end{axis}
	\end{tikzpicture}
\begin{tikzpicture} 
	\begin{axis}[
	name=,
	title={Czech-CAC},
        xtick style={draw=none},
        ytick style={draw=none},
        xtick={1,2,3,4,5,6,7},
        xticklabels={1,2,3,4,5,6,$\ge$7},
	typeset ticklabels with strut,
	legend pos=south west,
	ymajorgrids=false,
        ymax=1.0,
        ymin={0.85},
	grid style=dashed,
	height=4cm
	]
		\addplot[
		color=green,
		mark=*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.983029434)
			(2, 0.952917936)
			(3, 0.937731522)
			(4, 0.943788645)
			(5, 0.874576271)
			(6, 0.957446809)
			(7, 1.0)
};
		\addplot[
		color=cyan,
		mark=triangle*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.982858704)
			(2, 0.953082558)
			(3, 0.937202328)
			(4, 0.943226532)
			(5, 0.888135593)
			(6, 0.957446809)
			(7, 1.0)
};
		\addplot[
		color=red,
		mark=square*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.986000137)+=(0,0.00134856026285523)-=(0,0.00134856026285523)
			(2, 0.964029961)+=(0,0.003317290752570475)-=(0,0.003317290752570475)
			(3, 0.951490563)+=(0,0.005607453201193778)-=(0,0.005607453201193778)
			(4, 0.95727937)+=(0,0.009489989645509947)-=(0,0.009489989645509947)
			(5, 0.905084746)+=(0,0.03403245585016136)-=(0,0.03403245585016136)
			(6, 0.957446809)+=(0,0.07333263763454309)-=(0,0.07333263763454309)
			(7, 1.0)+=(0,0.3367274202349047)-=(0,0.3367274202349047)
};
		\addplot[
		color=brown,
		mark=diamond*,
                mark size=1,
                error bars/.cd, y dir=both, y explicit,
		]
		coordinates {
			(1, 0.986341597)
			(2, 0.948390814)
			(3, 0.927147645)
			(4, 0.937043283)
			(5, 0.881355932)
			(6, 0.936170213)
			(7, 1.0)
};

        \end{axis}
	\end{tikzpicture}
                 \caption{\label{fig:bpe_lens} Per-language accuracy
     on tokens with different numbers of BPE components, for the
     finetuning training regime. The last data point on the $x$-axis
     refers to all tokens composed of seven or more BPE tokens.  We
     indicate the method by encoding {\color{brown} First as brown},
     {\color{green} summation as green}, {\color{blue} averaging as
     blue} and {\color{red} RNN as red}. The accuracy is given on the
     $y$-axis. We show the Agresti-Coull approximation of a
     95\%-confidence interval for the RNN method
     \citep{agresti1998approximate}.  We do not show the intervals for
     other methods to avoid excessive clutter.}
	\end{figure}
	
	\section{Discussion}

%    \subsection{BPE composition methods}
    % For our experiments
    % We see that
        For predicting morphological features, the RNN method is more
     effective than the other proposed methods (summing, averaging or
     taking the first BPE token). This holds regardless of training
     regime (finetuning versus feature extraction) and across
     languages with different BPE per word ratios.
    
                 As we see it, the advantage of the RNN over
     commutative methods (Sum, Mean) and taking the first BPE token is
     that it can take the order of elements into account. In broad
     terms, information about the order of elements in morphology
     allows a system to determine what is a stem, prefix, or suffix.
      Thus allowing a model to collect more predictive information
     from token embeddings.

                 We can suspect that the average BPE per word ratio in
     a language affects the performance of the composition method
     used. To further control this variable, in \cref{fig:scatter_len}
     we plot the average number of BPE tokens per word in each
     language ($x$-axis), and compare this average against the gain in
     accuracy yielded by using the RNN method over summation
     ($y$-axis).  For finetuning we see that in general the average
     number of BPE tokens does not matter that much. The two cases
     where it does matter is for Turkish and Basque, where we see a
     substantial improvement of about $3$ percentage points. We note
     however that these are also the languages with the lowest amount
     of training data.
    %
        For the other languages the improvements lie in the range $.6$
     to $1.2$ percentage points. This indicates that when finetuning,
     the model can provide information that allows commutative methods
     to properly compose BPE tokens. However, looking at bare feature
     extraction we see that there is a larger gap between the low
     BPE-ratio and the high BPE-ratio languages.
    %\adam{This indicates that while the
    % pretrained BPE weights contain predictive information, there is
    % still more to be extracted from them.}

        Our sample of languages contain both fusional and
     agglutinative languages, and the typology does not appear to have
     an effect in our experiments. We see about the same trends for
     the fusional languages with a high BPE per word ratio as the
     agglutinative languages.

       
\begin{figure}%{r}{8cm}
    \centering  
        \begin{tikzpicture}
    	\begin{axis}[
		name=,
		title={},
		height=7cm,
        width=9cm,
        legend pos=north west,
        xlabel={BPE per word ratio},
		]
            \addplot[
             color=red,
    	     mark=triangle*,
    	     only marks,
              mark size=1.5
    	     ]
        	 coordinates {
            (1.98,0.006000000000000005)
            (1.79,0.03299999999999992)
            (1.73,0.03799999999999992)
            (1.86,0.010000000000000009)
            (1.39,0.01100000000000001)
            (1.25,0.0030000000000000027)
            (1.75,0.007000000000000006)
            (1.77,0.01200000000000001)
            };
            \addlegendentry{Finetuning}
    
            \addplot[no marks, color=red] table [row sep=\\,y={create col/linear regression={y=Y}},forget plot] {
            X Y \\
            1.98 0.006000000000000005 \\
            1.79 0.03299999999999992 \\
            1.73 0.03799999999999992 \\
            1.86 0.010000000000000009 \\
            1.39 0.01100000000000001 \\
            1.25 0.0030000000000000027 \\
            1.75 0.007000000000000006 \\
            1.77 0.01200000000000001 \\
            };
    
            \addplot[
             color=blue,
    	     mark=diamond*,
    	     only marks,
              mark size=1.5
    	     ]
            coordinates {
            (1.98,0.05599999999999994)
            (1.79,0.05900000000000005)
            (1.73,0.027999999999999914)
            (1.86,0.06299999999999994)
            (1.39,0.014000000000000012)
            (1.25,0.009999999999999898)
            (1.75,0.051000000000000045)
            (1.77,0.051000000000000045)
            };
            \addlegendentry{Feature extraction}
    
            \addplot[no marks, color=blue] table [row sep=\\,y={create col/linear regression={y=Y}},forget plot] {
            X Y \\
            1.98 0.05599999999999994 \\
            1.79 0.05900000000000005 \\
            1.73 0.027999999999999914 \\
            1.86 0.06299999999999994 \\
            1.39 0.014000000000000012 \\
            1.25 0.009999999999999898 \\
            1.75 0.051000000000000045 \\
            1.77 0.051000000000000045 \\
            };
    %\legend{Finetuning, Feature extraction}
        	\end{axis}
        \end{tikzpicture}
        \caption{The difference in accuracy between summation and RNN plotted against average number of BPE
     tokens per word in all languages, with a linear regression line.}
    \label{fig:scatter_len}
    \end{figure}

    \subsection{First method}

            The idea behind the First method is that the Transformer is
     sufficiently powerful to pool the relevant information into the
     first BPE token embedding.
    %
                            However, our experiments reveal that it is
     less efficient than any other method we tested for morphological
     sequence classification across languages. We see in
     \cref{tab:results_tokens} that the method is, on average, $.4$
     and $1$ percentage points lower than the next lowest scoring
     method for finetuning and feature extraction respectively.
     % 
             This effect is further enhanced when we consider the
     accuracy of words composed of more than two BPE tokens in
     \cref{tab:results_large_tokens}, where the difference is $1.6$
     and $2.7$ points, compared against the next lowest scoring
     method, for finetuning and feature extraction respectively. When
     we compare the performance against the RNN this difference only
     increases, showing a gain of $3.2$ percentage points and $7.7$
     points for finetuning and feature extraction respectively.

                             While the First method may be effective,
     primarily because of the expressivity of the Transformer
     architecture, the method forces the model to push the predictive
     information of several BPE token embeddings into the first one. This
     puts an additional burden on the Transformer model, and we believe that this
     is the reason for the performance degradation which we observe.
     Besides, putting this burden on the model is not necessary: pooling
     information from several BPE embeddings can be done effectively using additional layers.


    \subsection{Sum and Mean}
                When we consider the commutative methods of combining
     token embeddings, summation or averaging, we see no clear advantage
     for either of them over the other one, when doing
     finetuning. However, when extracting features only we see hints
     that summation is more effective than averaging. For feature
     extraction, summation is $.5$ percentage points better than
     averaging, and words composed of two or more BPE tokens exhibit
     an advantage of $.9$ point for summation.
    
                This discrepancy suggests that by averaging, we are
     removing some predictive information from the pretrained BPE
     token embeddings, that is, by reducing the values in the token embeddings
     uniformly across a sequence of token embeddings we lose useful
     information.
        We believe that some token embeddings contain more predictive
     information than others, and by summing them we retain all the
     information.
      %
                         But when we finetune, the difference between
     summing and averaging almost disappears: the model appears to
     learn how to distribute the information uniformly across the
     token embeddings that compose a word and is thus able to retain the
     information better. Interestingly, the model learns to distribute
     the information across multiple BPE token embeddings more
     efficiently than pushing the information into the first
     token. This is shown by the large difference in accuracy between
     finetuning and feature extraction for the First and averaging
     method.


    \subsection{Parameterization of First, Sum and Mean}

            One question that arises when looking at
     \cref{fig:bpe_lens}, specifically considering the performance on
     words composed of only one BPE token is the following: can the
     superiority of the RNNs be attributed to its ability to take
     context into account, or simply to containing more parameters
     and extra layers?
    %
         We would expect that for the words with only one BPE token,
     the performance of the model would be the same for all
     methods. For practical reasons, we push all word embeddings
     through an RNN, effectively doing a non-linear transformation
     with $\mathsf{tanh}$ activations on the words composed of only
     one BPE token. % for each of them.
    %
                Typically, the difference in accuracy between various
     methods for one-BPE-token words is small (barely visible in
     \cref{fig:bpe_lens}). But for example in Finnish, we see a larger
     difference. Although in general if we perform better on longer
     words consisting of BPE tokens that also appear as words in the
     data, we could also expect the performance to be better for
     words of BPE length one, because we will have more accurate
     representations of the contextual words.

    \begin{table}%[h]
	%\small
	\centering
	\begin{tabular}{l|c|cccc|cccc}
		& & \multicolumn{4}{c}{Finetuning} & \multicolumn{4}{c}{Feature extraction} \\
		Treebank & Baseline & First & Sum & Mean & RNN & First & Sum & Mean & RNN \\
		\hline
		Basque-BDT      & .676 & .864 & .894 & .890 & \textbf{.901} & .772 & .793 & .794 & \textbf{.834} \\
		Finnish-TDT     & .751 & .958 & .959 & .961 & \textbf{.965} & .857 & .856 & .855 & \textbf{.899} \\
		Turkish-IMST    & .620 & .850 & .875 & .867 & \textbf{.884} & .742 & .722 & .729 & \textbf{.775} \\
		Estonian-EDT    & .740 & .956 & .958 & .958 & \textbf{.961} & .865 & .856 & .853 & \textbf{.901} \\
		Spanish-AnCora  & .842 & .978 & .977 & .978 & \textbf{.979} & .953 & .954 & .952 & \textbf{.962} \\
		Arabic-PADT     & .770 & .949 & .945 & .947 & \textbf{.951} & .925 & .923 & .920 & \textbf{.936} \\
		Czech-CAC       & .771 & .969 & .972 & .972 & \textbf{.975} & .873 & .887 & .881 & \textbf{.924} \\
		Polish-LFG      & .657 & .957 & .953 & .955 & \textbf{.959} & .832 & .844 & .840 & \textbf{.878} \\
        \hline
        Average         & .728 & .935 & .942 & .941 & \textbf{.946} & .852 & .854 & .853 & \textbf{.888} \\
        %Difference           & - &  +.002 & +.005 & +.005 & - & +.006 & -.002 & +.002 & - \\
	\end{tabular}
        	\caption{\label{tab:commutative-parameters} The accuracy
     of morphological tagging when we parameterize the First, Sum and Mean method
     with a non-linear transformation layer.}
    \end{table}

             We test this hypothesis by parameterizing the First, Sum,
     and Mean method. Essentially, we need to increase the
     capabilities of these methods.
    %
            This is done by passing all BPE token embeddings through a
     non-linear transformation with $\mathsf{ReLU}$ activation before
     we compute the Sum, Mean, or select the first BPE-token.
    %
                    Our experiment, whose results are shown in
     \cref{tab:commutative-parameters}, shows that while adding
     parameters to the First, Sum, and Mean method generally improve their
     performance slightly, ranging between a change of  $-0.2$ and
     $+0.6$ percentage points, but their performance never exceeds that
     of the RNN method.

    

    \section{Conclusions and Future Work}


                In conclusion, our results indicate that using an RNN
     to compose word representations from token representations,
     obtained from a large Transformer model, is more efficient than
     two commutative methods, summing and averaging, and also more
     effective than letting a Transformer model automatically pool the
     predictive word-level information into the first BPE token
     embedding.
    %
                We show this for the task of morphological sequence
     classification, in eight different languages with varying
     morphology and word-lengths in term of BPE tokens, as well as for
     two training regimes, finetuning and feature extraction.
     
        In future work, we want to continue experimenting with the
     different BPE token embedding composition methods, specifically
     looking at more complex syntactic and semantic tasks, such as
     dependency and/or constituency parsing, semantic role labeling,
     named entity recognition, and natural language inference.
    %
            We also wish to run our experiments on the hundreds of
     available UD treebanks to improve the robustness of our results.
    
	\section*{Acknowledgments}
            The research reported in this paper was supported by grant
     2014-39 from the Swedish Research Council, which funds the Centre
     for Linguistic Theory and Studies in Probability (CLASP) in the
     Department of Philosophy, Linguistics, and Theory of Science at
     the University of Gothenburg.

	% include your own bib file like this:
	\bibliographystyle{coling}
	\bibliography{coling2020}
	
\end{document}
